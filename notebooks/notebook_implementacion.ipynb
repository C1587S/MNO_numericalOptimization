{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementación de los métodos\n",
    "\n",
    "En este _notebook_ se plantea la solución del problema utilizando los algoritmos planteados. Este archivo está autocontenido, sin embargo, la implementación principal se realiza con enfoque _modular_.\n",
    "\n",
    "**Nota:** Esta implementación se basa en material y actividades impartidas por los profesores de los cursos de [Métodos Numéricos y optimización](https://github.com/ITAM-DS/analisis-numerico-computo-cientifico/blob/master/temas/IV.optimizacion_convexa_y_machine_learning/4.3.Regresion_logistica_R.ipynb) (2010-I) (Prof. Erick Palacios Moreno) y Aprendizaje de Máquina (2019-II) (Prof. Rodrigo Mendoza Smith).\n",
    "\n",
    "\n",
    "A continuación se describe el planteamiento del problemna, y su implementación con el set de datos utilizado. Una explicación más detallada se realiza en el informe (en formato PDF) de este proyecto.\n",
    "\n",
    "## Problema de regresión Logística\n",
    "\n",
    "Inicialmente se considera un conjunto de datos que incorpora una variable output $y_{i}$ asociada a la supervivencia o no del paciente ${i}$ con virus del ébola, y ${j}$ variables explicativas asociadas, $x_{i,j}$. Matemáticamente, este conjunto se define de la siguiente manera: $\\mathcal{D}=\\left\\{ \\left(x_{i},y_{i}\\right)\\in\\mathbb{R}^{p}\\times\\left\\{ 0,1\\right\\} :i\\in\\left[m\\right]\\right\\} $.\n",
    "\n",
    "El método de _regresión logística_ asume que $Pr\\left[y_{i}\\mid x_{i},\\beta\\right]\\sim Bernoulli\\left(\\mu_{i}\\right)$\n",
    "con los siguientes supuestos sobre la media, $\\mu_{i}$:\n",
    "\n",
    "$$\n",
    "\\mu_{i}=\\sigma\\left(\\beta^{T}x_{i}\\right) \\label{eq-3.1} \\tag{1}\n",
    "$$\n",
    "$$\n",
    "\\sigma(z)=\\left(1+\\exp\\left(-z\\right)\\right)^{-1} \\label{eq-3.2} \\tag{2}\n",
    "$$\n",
    "\n",
    "donde $\\beta\\in\\mathbb{R}^{p}$. \n",
    "\n",
    "\n",
    "Dado lo anterior, nuestro problema es encontrar un modelo tal que $\\hat{\\beta}\\in\\mathbb{R}^{p}$ explica de la mejor manera posible a $\\mathcal{D}$. \n",
    "\n",
    "Para lograr lo anterior, debemos estimar el conjunto de parámetros $\\hat{\\beta}$ para modelar $Pr\\left[y\\mid x,\\hat{\\beta}\\right]$ y predecir la etiqueta $\\hat{y}\\in\\left\\{ 0,1\\right\\} $ de un nuevo\n",
    "dato $x$ por medio de:\n",
    "\n",
    "$$\n",
    "\\hat{y}=\\begin{cases}\n",
    "1 & si\\,\\sigma\\left(\\hat{\\beta}^{T}x\\right)\\geq0.5\\\\\n",
    "0 & si\\,\\sigma\\left(\\hat{\\beta}^{T}x\\right)<0.5\n",
    "\\end{cases}\\label{eq-3.3} \\tag{3}\n",
    "$$\n",
    "\n",
    "la función de pérdida que queremos minimizar en este problema corresponde a la _log-verosimilitud negativa_ , que está dada por:\n",
    "\n",
    "$$\n",
    "F(\\beta):=LVN(\\beta)=-\\sum_{i=1}^{m}\\left[y_{i}log\\mu_{i}+(1-y_{i})log(1-\\mu_{i})\\right]\\label{eq-3.4} \\tag{4}\n",
    "$$\n",
    "\n",
    "\n",
    "Una vez planteado lo anterior, queremos encontrar $\\hat{\\beta}$ por medio de métodos numéricos de optimización de tal forma que se minimize ([4](#mjx-eqn-eq1)) para el conjunto de datos dado.\n",
    "\n",
    "\n",
    "_En los siguientes fragmentos de código se realiza el planteamiento del problema, desde la importación de datos hasta el proceso de entrenamiento del modelo utilizando distintos algoritmos de optimización que se explican con brevedad._\n",
    "\n",
    "\n",
    "---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# librerías\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importación de datos\n",
    "\n",
    "En esta sección se importa y transforma los datos, con el fin de obtener el conjunto $\\mathcal{D}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url=\"https://raw.githubusercontent.com/afcarl/ebola-imc-public/master/data/kenema/test/pres-kgh/imputation-50.csv\"\n",
    "df_raw=pd.read_csv(url,sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw.head()\n",
    "# df[df.isnull().any(axis=1)] - no hay NAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verificar tipo de variables \n",
    "df_raw.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resumen de las variables\n",
    "df_raw.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Algunas notas sobre los datos**\n",
    "\n",
    "- _Variables numéricas_ :\n",
    "    - `CT`\n",
    "    - `TEMP`\n",
    "    \n",
    "- _Variables categóricas_ :\n",
    "    - `AGE`: se establecen 3 grupos de edades utilizando el percetil 25 (22 años), percentil 50 (36 años) y percentil 75 (45 años)\n",
    "    - `HEADCH`: binaria\n",
    "    - `BLEED`: binaria\n",
    "    - `DIARR`: binaria\n",
    "    - `VOMIT`: binaria\n",
    "    - `PABD`: binaria\n",
    "    - `WEAK`: binaria\n",
    "    \n",
    "   \n",
    "- Para este conjunto de datos la variable `JAUN` no tiene variabilidad, por lo tanto no es una variable, y se omite.\n",
    "\n",
    "Dado lo anterior, se ajusta el set de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ajustes en df_raw \n",
    "df_proc = df_raw\n",
    "df_proc['INTER_AGE'] = \"NA\"\n",
    "\n",
    "df_proc.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ajustes en df_raw \n",
    "df_proc = df_raw\n",
    "\n",
    "# para la variable edad se crean cuatro categorías\n",
    "age_p25 = math.ceil(df_proc['AGE'].quantile(.25))\n",
    "age_p50 = math.ceil(df_proc['AGE'].quantile(.50))\n",
    "age_p75 = math.ceil(df_proc['AGE'].quantile(.75))\n",
    "\n",
    "df_proc['INTER_AGE'] = \"NA\"\n",
    "df_proc.loc[(df_proc['AGE'] <= age_p25), 'INTER_AGE'] = 1\n",
    "df_proc.loc[(df_proc['AGE'] > age_p25) & (df_proc['AGE'] <= age_p50), 'INTER_AGE'] = 2\n",
    "df_proc.loc[(df_proc['AGE'] > age_p50) & (df_proc['AGE'] <= age_p75), 'INTER_AGE'] = 3\n",
    "df_proc.loc[(df_proc['AGE'] > age_p75), 'INTER_AGE'] = 4\n",
    "\n",
    "## one hot encoding\n",
    "enc = OneHotEncoder(handle_unknown='ignore')\n",
    "enc_df = pd.DataFrame(enc.fit_transform(df_proc[['INTER_AGE']]).toarray())\n",
    "enc_df = enc_df.rename(columns={0: f\"hasta{age_p25}\", 1: f\"entre{age_p25+1}y{age_p50}\", 2: f\"entre{age_p50+1}y{age_p75}\", 3:f\"mayor{age_p75}\"})\n",
    "# merge with main df bridge_df on key values\n",
    "df_proc = df_proc.join(enc_df)\n",
    "\n",
    "# se asignan como categoricas a las binarias, incluido el output\n",
    "bin_vars = ['OUT', 'HEADCH', 'BLEED', 'DIARR', 'JAUN', 'VOMIT',\n",
    "       'PABD', 'WEAK', 'INTER_AGE', f\"hasta{age_p25}\", f\"entre{age_p25+1}y{age_p50}\", f\"entre{age_p50+1}y{age_p75}\", f\"mayor{age_p75}\"]\n",
    "for var in bin_vars:\n",
    "    df_proc[var] = df_proc[var].astype('category')\n",
    "    \n",
    "# se omiten las variables JAUN, AGE, INTER_AGE\n",
    "del_vars = [\"JAUN\", \"AGE\", \"INTER_AGE\"]\n",
    "for var in del_vars:\n",
    "    df_proc = df_proc.drop(var, axis=1)    \n",
    "    \n",
    "# se comprueban los tipos de variable\n",
    "df_proc.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_proc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Planteamiento del problema de regresión\n",
    "\n",
    "A continuación se plantea el código que computa las ecuaciones ([1](#mjx-eqn-eq1)), ([2](#mjx-eqn-eq1)) y ([4](#mjx-eqn-eq1)), planteadas inicialmente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoide(z):\n",
    "    '''\n",
    "    Devuelve el sigmoide de un vector\n",
    "        ** Parámetros:\n",
    "            - z (vec): vector numérico de m entradas\n",
    "        ** Salidas\n",
    "            - (float64) valor entre -1 y 1\n",
    "    '''\n",
    "    return 1/(1+np.exp(-z).astype(float))\n",
    "    \n",
    "def calc_mu(X,beta):\n",
    "    '''\n",
    "    Calcula la media para una variable aleatoria con distribución bernoulli.\n",
    "        ** Parámetros:\n",
    "            - X (mat): matriz de mxp entradas\n",
    "            - beta (vec): vector de p entradas\n",
    "        ** Salidas\n",
    "            - mu (vec): vector de m entradas\n",
    "    '''\n",
    "    a = np.matmul(beta,np.transpose(X))\n",
    "    mu = sigmoide(a)\n",
    "\n",
    "    return mu\n",
    "    \n",
    "def f(X,y,beta):\n",
    "    '''\n",
    "    Función que computa la log-verosimilitud negativa\n",
    "    ** Parámetros:\n",
    "        - X (mat): matriz de mxp entradas\n",
    "        - y (vec): vector de de m entradas de la variable output\n",
    "        - beta (vec): vector de p entradas\n",
    "    ** Salidas\n",
    "        - lvn (int): log-verosimilitud negativa\n",
    "    '''\n",
    "    prob = calc_mu(X,beta)\n",
    "    # log-verosimilitud negativa \n",
    "    lvn = -sum(y*np.log(prob)+(1-y)*(np.log(1-prob)))\n",
    "    return lvn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reescribiendo la ecuación de la función de pérdida ([4](#mjx-eqn-eq1)), tenemos:\n",
    "\n",
    "$$F(\\beta)=- \\sum_{i=1}^{m}[y_i log\\mu_i + (1-y_i)log(1-\\mu_i)]$$\n",
    "\n",
    "Las expresiones correspondientes al gradiente y a la matriz hessiana asociados a este problema, se plantean a continuación:\n",
    "\n",
    "\\begin{align}\n",
    "\\nabla F(\\beta) & =\\frac{d}{d\\beta}F(\\beta)\\nonumber \\\\\n",
    " & =\\sum_{i}\\left(\\mu_{i}-y_{i}\\right)x_{i}\\nonumber \\\\\n",
    " & =\\boldsymbol{X}^{T}\\left(\\boldsymbol{\\mu}-\\boldsymbol{y}\\right)\\label{eq:gradient}\n",
    "\\end{align}\n",
    "\n",
    "Por otro lado, la ecuación que describe la matrix Hessiana es la siguiente:\n",
    "\n",
    "\\begin{align}\n",
    "\\nabla^{2}F(\\beta) & =\\frac{d}{d\\beta}\\nabla F\\left(\\beta\\right)^{T}\\nonumber \\\\\n",
    " & =\\sum_{i}\\left(\\nabla_{\\beta}\\mu_{i}\\right)x_{i}^{T}\\nonumber \\\\\n",
    " & =\\sum_{i}\\mu_{i}\\left(1-\\mu_{i}\\right)x_{i}x_{i}^{T}\\nonumber \\\\\n",
    " & =\\boldsymbol{X^{T}SX}\\label{eq:hessian}\n",
    "\\end{align}\n",
    "\n",
    "donde $\\boldsymbol{S}\\triangleq diag\\left(\\mu_{i}\\left(1-\\mu_{i}\\right)\\right)$.\n",
    "Como es resaltado por Murphy (2012), es definida positiva, lo que implica que ([4](#mjx-eqn-eq1)) es convexa\n",
    "y tiene un mínimo global."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradiente_f(X,y,beta):\n",
    "    '''\n",
    "    Calcula el gradiente asociado la log-verosimilitud negativa del problema de regresión logística\n",
    "        ** Parámetros:\n",
    "            - X (mat): matriz de mxp entradas\n",
    "            - y (vec): vector de de m entradas de la variable output\n",
    "            - beta (vec): vector de p entradas\n",
    "        ** Salidas\n",
    "            - grad (vec): vector de m entradas\n",
    "    '''\n",
    "    mu=calc_mu(X,beta)\n",
    "    grad = np.matmul(np.transpose(X), mu-y)\n",
    "    return grad\n",
    "\n",
    "def hessiana_f(X,y,beta):\n",
    "    '''\n",
    "    Calcula la matriz Hessiana asociada a la log-verosimilitud negativa del problema de regresión logística\n",
    "        ** Parámetros:\n",
    "            - X (mat): matriz de mxp entradas\n",
    "            - y (vec): vector de de m entradas de la variable output\n",
    "            - beta (vec): vector de p entradas\n",
    "        ** Salidas\n",
    "            - grad (vec): vector de m entradas\n",
    "    '''\n",
    "    mu=calc_mu(X,beta)\n",
    "    S=np.diag(mu*(1-mu))\n",
    "    hes=np.matmul(np.transpose(X),np.matmul(S,X))\n",
    "    return hes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(x):\n",
    "    return x/np.sqrt(sum(x*x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clasifica(X, beta_hat):\n",
    "    mu=calc_mu(X,beta_hat)\n",
    "    yhat=mu\n",
    "    yhat[mu<.5]=0\n",
    "    yhat[mu>=.5]=1\n",
    "    return yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def descent_direction(X, y, beta, method=\"max\"):\n",
    "    '''\n",
    "    This function determines the direction of the descent pk=-inv(Bk) grad f\n",
    "    '''\n",
    "    if(method==\"max\"):\n",
    "        pk=gradiente_f(X,y,beta)\n",
    "    \n",
    "    elif(method==\"newton\"):\n",
    "        grad=gradiente_f(X,y,beta)\n",
    "        hess=hessiana_f(X,y,beta)\n",
    "        inv_hess=np.linalg.inv(hess)\n",
    "        pk=np.matmul(inv_hess,grad)\n",
    "    \n",
    "    \n",
    "    return normalize(pk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y,lr=.1, tol=10**(-6), max_iter=10**5, method=\"max\"):\n",
    "    '''\n",
    "    Devuelve vector de parámetros beta (px1) resultante del proceso de optimización por descenos de gradiente\n",
    "        ** Parámetros:\n",
    "            - X (mat): matriz de mxp entradas\n",
    "            - y (vec): vector de de m entradas de la variable output\n",
    "            - lr (float64): tasa de aprendizaje\n",
    "            - tol (float64): criterio de convergencia\n",
    "            - max_iter (int): número máximo de iteraciones\n",
    "            - method (str): método que determina la dirección de descenso\n",
    "                opciones:\n",
    "                    - max: método de descenso\n",
    "                    - newton: método de Newton\n",
    "                    - \n",
    "            \n",
    "        ** Salidas\n",
    "            - beta_new (vec): vector de p entradas con parámetros que minimizan la función de pérdida\n",
    "    '''\n",
    "    \n",
    "    #inicializa\n",
    "    iteraciones=0\n",
    "    \n",
    "    #inicializamos beta aleatoria\n",
    "    beta=np.random.uniform(0,1,X.shape[1])\n",
    "    \n",
    "    #primera iteracion\n",
    "    pk = descent_direction(X, y, beta, method)\n",
    "    beta_new= beta - lr*pk\n",
    "    \n",
    "    #condición de paro.\n",
    "    #El cambio total es menor que la tolerancia\n",
    "    while ((abs(f(X,y,beta) - f(X,y,beta_new))>tol) & (iteraciones<max_iter)):\n",
    "        iteraciones+=1 #contador de ciclo\n",
    "        \n",
    "        beta = beta_new\n",
    "        pk = descent_direction(X,y,beta,method)\n",
    "        beta_new = beta - lr*pk\n",
    "\n",
    "        #if iteraciones>max_iter:\n",
    "        #    break\n",
    "    \n",
    "    print(\"iteraciones=\",iteraciones)\n",
    "    return beta_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nota:** Implementación tomando datos anteriores - se debe corregir la matriz de diseño"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "url=\"https://raw.githubusercontent.com/afcarl/ebola-imc-public/master/data/kenema/test/pres-kgh/imputation-50.csv\"\n",
    "df=pd.read_csv(url,sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "data =df.to_numpy()\n",
    "y = data[:,0]\n",
    "X = data[:,1:]\n",
    "x_train, x_test, y_train, y_test=train_test_split(X,y,test_size=.2)\n",
    "\n",
    "#scale data\n",
    "scaler=MinMaxScaler()\n",
    "x_train=scaler.fit_transform(x_train)\n",
    "x_test=scaler.fit_transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteraciones= 644\n",
      "beta_hat= [-9.27894027  5.41544927 24.74342114 -6.53123606  0.2179865   2.8421769\n",
      "  0.4173048  10.0601313   5.32053725  4.00423041]\n",
      "Error de clasificacion= 22.73 %\n"
     ]
    }
   ],
   "source": [
    "beta_hat=gradient_descent(x_train,y_train)\n",
    "yhat=clasifica(x_test,beta_hat)\n",
    "\n",
    "print(\"beta_hat=\", beta_hat)\n",
    "print(\"Error de clasificacion=\",round(100*sum(abs(y_test-yhat))/len(yhat),2),\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Método de Newton (no está funcionando)\n",
    "beta_hat=gradient_descent(x_train,y_train, method=\"newton\")\n",
    "yhat=clasifica(x_test,beta_hat)\n",
    "\n",
    "print(\"beta_hat=\", beta_hat)\n",
    "print(\"Error de clasificacion=\",round(100*sum(abs(y_test-yhat))/len(yhat),2),\"%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
